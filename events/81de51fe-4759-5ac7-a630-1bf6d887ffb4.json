{
  "id": 587,
  "guid": "81de51fe-4759-5ac7-a630-1bf6d887ffb4",
  "logo": "",
  "date": "2019-12-27T14:00:00+01:00",
  "start": "14:00",
  "duration": "00:45",
  "room": "WikiPaka WG: Esszimmer",
  "slug": "LTYSYX",
  "url": "https://cfp.verschwoerhaus.de/36c3/talk/LTYSYX/",
  "title": "Datensammlung leicht gemacht",
  "subtitle": "",
  "track": "Science",
  "type": "Talk",
  "language": "de",
  "abstract": "Vorstellung eines Ansatzes zur Verkn\u00fcpfung von OpenData aus verschiedenen Quellen",
  "description": "Ob OpenData oder Datenjournalismus: Wer Daten aus verschiedenen Quellen zusammenf\u00fchrt (APIs, Exceldateien, Datenbanken, etc.), ist oft mit dem Problem konfrontiert, diese zueinander in Beziehung zu setzen, zu analysieren oder eine aufbereitete Version der Daten als Mashup zu pr\u00e4sentieren (z. B. als Website). Mitunter m\u00f6chte man au\u00dferdem \u00fcber einen l\u00e4ngeren Zeitraum Daten erheben, um Ver\u00e4nderungen zu verfolgen und ggf. darauf zu reagieren.\r\n\r\nIch m\u00f6chte einen Ansatz vorstellen, wie man dieses Problem mit einem weitestgehend generischen Ansatz l\u00f6sen kann. Dieser Ansatz, der sich Data Vault 2.0 nennt, ist in Industrie und Wirtschaft bereits verbreitet, in anderen Teilen der datenverarbeitenden Community offenbar aber unbekannt. Dies m\u00f6chte ich \u00e4ndern.\r\n\r\n## Beispiele\r\nVerkn\u00fcpfung offener Datens\u00e4tze, Verkn\u00fcpfung von Daten aus \u00f6ffentlich verf\u00fcgbaren Schnittstellen\r\n\r\n## Themenfelder\r\nOpen Data, Open Government, Data-driven Journalism (DDJ)",
  "recording_license": "",
  "do_not_record": true,
  "persons": [
    {
      "id": 84,
      "code": "DGTUF3",
      "public_name": "cyroxx",
      "biography": "Datenreisender aus dem Chaostreff Potsdam. \r\nOffene Daten, FOSS, Politik.\r\nMacht viel mit Python.\r\n\r\nGithub: https://github.com/cyroxx\r\nMastodon: https://chaos.social/@cyroxx\r\nTwitter: https://twitter.com/cyroxx",
      "answers": []
    }
  ],
  "links": [],
  "attachments": [],
  "answers": []
}